# -*- coding: utf-8 -*-
"""Gait_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IE4dIKbtV-nPFbR7B9uDQqLinri3Vndd

**Importing the required Libraries and Packages**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, f1_score, log_loss
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from lightgbm import LGBMClassifier
import numpy as np

"""**Reading the Dataset | Accelerometer Readings [ X , Y, Z ] - Chest, Right Hand, Right Leg**"""

gait_chest = pd.read_excel("C:/Users/Akash R H/OneDrive/Desktop/FYP_2025/DATASET/GAIT/Accel_Chest.xlsx")
gait_hand = pd.read_excel("C:/Users/Akash R H/OneDrive/Desktop/FYP_2025/DATASET/GAIT/Accel_Hand.xlsx")

gait_chest.head(1)

gait_chest.info()

gait_hand.head(1)

gait_hand.info()

gait_chest = gait_chest[['PID','Status','Cleaned_Features']]
gait_hand = gait_hand[['PID','Status','Cleaned_Features']]

print(gait_chest.shape)
gait_chest.head()

print(gait_hand.shape)
gait_hand.head()

"""**Status of the Dataset | 1 - Parkinson | 0 - Healthy**"""

gait_chest.isnull().sum()

gait_hand.isnull().sum()

gait_chest['Status'].value_counts()
gait_hand['Status'].value_counts()

def plot_distributions(data, features):
    num_columns = 3  # Adjust the number of columns as needed
    num_plots = len(features)
    num_rows = (num_plots // num_columns) + (num_plots % num_columns > 0)

    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 3))
    axes = axes.flatten()  # Flatten in case there are empty subplots

    for i, col in enumerate(features):
        sns.histplot(data[col], ax=axes[i], kde=True)
        axes[i].set_title(col)

    # Hide any empty subplots
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()

features = gait_chest.select_dtypes(include=['float', 'int', 'string']).columns #ignore
plot_distributions(gait_chest, features)

def plot_distributions(data, features):
    num_columns = 3  # Adjust the number of columns as needed
    num_plots = len(features)
    num_rows = (num_plots // num_columns) + (num_plots % num_columns > 0)

    fig, axes = plt.subplots(num_rows, num_columns, figsize=(15, num_rows * 3))
    axes = axes.flatten()  # Flatten in case there are empty subplots

    for i, col in enumerate(features):
        sns.histplot(data[col], ax=axes[i], kde=True)
        axes[i].set_title(col)

    # Hide any empty subplots
    for j in range(i + 1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()

features = gait_hand.select_dtypes(include=['float', 'int', 'string']).columns #ignore
plot_distributions(gait_hand, features)

X = gait_chest.drop(columns=['Status'], axis =1)
Y = gait_chest['Status']

A = gait_hand.drop(columns=['Status'], axis =1)
B = gait_hand['Status']

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 3)
A_train, A_test, B_train, B_test = train_test_split(A, B, test_size=0.2, random_state = 3)

(X.shape,X_train.shape,X_test.shape)

(A.shape,A_train.shape,A_test.shape)

"""**MODEL TRAINING**"""

Model = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Support Vector Machine": SVC(probability=True, random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "Random Forest": RandomForestClassifier(random_state=42),
    "Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42),
    "AdaBoost": AdaBoostClassifier(random_state=42),
    "Gradient Boosting": GradientBoostingClassifier(random_state=42),
    "Linear Discriminant Analysis": LinearDiscriminantAnalysis(),
    "Quadratic Discriminant Analysis": QuadraticDiscriminantAnalysis(),
    "LightGBM": LGBMClassifier(random_state=42)
}

"""**Trainig for CHEST Data**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, precision_score, f1_score,
                             log_loss, mean_squared_error, recall_score)
import warnings

# Suppress warnings globally
warnings.filterwarnings("ignore")

# Function to train and evaluate the model
def train_and_evaluate_model(model_name, model, X_train, X_test, Y_train, Y_test):
    # Convert the string values in 'Cleaned_Features' to numerical format
    X_train_numeric = np.array([list(map(float, row.split(", "))) for row in X_train])  # Use X_train directly
    X_test_numeric = np.array([list(map(float, row.split(", "))) for row in X_test])  # Use X_test directly

    # Train the model
    if model_name == "LightGBM":
        model.set_params(verbose=-1)  # Suppress LightGBM-specific warnings

    model.fit(X_train_numeric, Y_train)

    # Make predictions
    Y_pred = model.predict(X_test_numeric)

    # Predict probabilities if the model supports it
    try:
        Y_prob = model.predict_proba(X_test_numeric)
    except AttributeError:
        Y_prob = None

    # Calculate metrics
    accuracy = accuracy_score(Y_test, Y_pred)
    f1 = f1_score(Y_test, Y_pred)
    precision = precision_score(Y_test, Y_pred)
    recall = recall_score(Y_test, Y_pred)
    mse = mean_squared_error(Y_test, Y_pred)

    if Y_prob is not None:
        try:
            ll = log_loss(Y_test, Y_prob)
        except ValueError:
            ll = np.nan
    else:
        ll = np.nan

    # Create a results dictionary
    results_chest = {
        "Model": model_name,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Precision": precision,
        "Recall": recall,
        "Mean Squared Error": mse,
        "Log Loss": ll
    }

    # Generate the confusion matrix
    cm = confusion_matrix(Y_test, Y_pred)

    return results_chest, cm

# Collect results from all models
results_list_chest = []
confusion_matrices = {}

for model_name, model in Model.items():
    print(f"Training model: {model_name} on gait_chest data")

    # Assuming 'Cleaned_Features' is a numerical column or preprocessed feature
    X_train_features = X_train['Cleaned_Features']  # Access 'Cleaned_Features' from X_train DataFrame
    X_test_features = X_test['Cleaned_Features']  # Access 'Cleaned_Features' from X_test DataFrame

    # Train and evaluate the model for gait_chest only
    results_chest, cm = train_and_evaluate_model(
        model_name, model, X_train_features, X_test_features, Y_train, Y_test
    )
    results_list_chest.append(results_chest)
    confusion_matrices[model_name] = cm

# Create a DataFrame for results
df_results_chest = pd.DataFrame(results_list_chest)
print("\nFinal Results Table:")
print(df_results_chest)

# Plot all confusion matrices in a 6x2 grid
num_models = len(confusion_matrices)
fig, axes = plt.subplots(4, 3, figsize=(12, 16))
axes = axes.flatten()

for idx, (model_name, cm) in enumerate(confusion_matrices.items()):
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[idx],
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    axes[idx].set_title(f" {model_name}")
    axes[idx].set_xlabel("Predicted")
    axes[idx].set_ylabel("Actual")

# Hide any unused subplots
for i in range(len(confusion_matrices), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming 'df_results_chest' contains the results for all models, with columns like 'Accuracy', 'F1 Score', etc.

# Define short forms for the models
model_shortforms = {
    "Logistic Regression": "LogReg",
    "Support Vector Machine": "SVM",
    "K-Nearest Neighbors": "KNN",
    "Decision Tree": "DT",
    "Random Forest": "RF",
    "Naive Bayes": "NB",
    "XGBoost": "XGB",
    "AdaBoost": "AdaBoost",
    "Gradient Boosting": "GB",
    "Linear Discriminant Analysis": "LDA",
    "Quadratic Discriminant Analysis": "QDA",
    "LightGBM": "LGBM"
}

# Apply short forms to the model names in the results DataFrame
df_results_chest["Model"] = df_results_chest["Model"].map(model_shortforms)

# Set the seaborn style for better aesthetics
sns.set(style="whitegrid", palette="muted", font_scale=1.2)

# Define the metrics to plot
metrics = ["Accuracy", "F1 Score", "Precision", "Recall", "Mean Squared Error", "Log Loss"]

# Plotting bar plots for each metric
fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # 2 rows, 3 columns of subplots
axes = axes.flatten()

# Loop through each metric to create a plot
for i, metric in enumerate(metrics):
    sns.barplot(x=df_results_chest["Model"], y=df_results_chest[metric], ax=axes[i], palette="viridis", edgecolor='black')
    axes[i].set_title(f"{metric} Comparison", fontsize=16, weight='bold')
    axes[i].set_xlabel("Model", fontsize=14, weight='bold')
    axes[i].set_ylabel(metric, fontsize=14, weight='bold')
    axes[i].set_ylim(0, 1)  # Setting y-axis limits for better comparison (for most metrics like accuracy, precision, etc.)

    # Rotate x-axis labels for better readability
    axes[i].tick_params(axis="x", rotation=45)

    # Adding horizontal gridlines
    axes[i].grid(True, axis='y', linestyle='--', alpha=0.7)

# Add overall figure title
fig.suptitle("Model Performance Comparison", fontsize=18, weight='bold')

# Adjust layout to avoid overlap
plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for the suptitle
plt.show()

"""**Training for HAND Data**"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, precision_score, f1_score,
                             log_loss, mean_squared_error, recall_score)
import warnings

# Suppress warnings globally
warnings.filterwarnings("ignore")

# Function to train and evaluate the model
def train_and_evaluate_model(model_name, model, A_train, A_test, B_train, B_test):
    # Convert the string values in 'Cleaned_Features' to numerical format
    A_train_numeric = np.array([list(map(float, row.split(", "))) for row in A_train])  # Use A_train directly
    A_test_numeric = np.array([list(map(float, row.split(", "))) for row in A_test])  # Use A_test directly

    # Train the model
    if model_name == "LightGBM":
        model.set_params(verbose=-1)  # Suppress LightGBM-specific warnings

    model.fit(A_train_numeric, B_train)

    # Make predictions
    B_pred = model.predict(A_test_numeric)

    # Predict probabilities if the model supports it
    try:
        B_prob = model.predict_proba(A_test_numeric)
    except AttributeError:
        B_prob = None

    # Calculate metrics
    accuracy = accuracy_score(B_test, B_pred)
    f1 = f1_score(B_test, B_pred)
    precision = precision_score(B_test, B_pred)
    recall = recall_score(B_test, B_pred)
    mse = mean_squared_error(B_test, B_pred)

    if B_prob is not None:
        try:
            ll = log_loss(B_test, B_prob)
        except ValueError:
            ll = np.nan
    else:
        ll = np.nan

    # Create a results dictionary
    results_hand = {
        "Model": model_name,
        "Accuracy": accuracy,
        "F1 Score": f1,
        "Precision": precision,
        "Recall": recall,
        "Mean Squared Error": mse,
        "Log Loss": ll
    }

    # Generate the confusion matrix
    cm = confusion_matrix(B_test, B_pred)

    return results_hand, cm

# Collect results from all models
results_list_hand = []
confusion_matrices = {}

for model_name, model in Model.items():
    print(f"Training model: {model_name} on gait_hand data")

    # Assuming 'Cleaned_Features' is a numerical column or preprocessed feature
    A_train_features = A_train['Cleaned_Features']  # Access 'Cleaned_Features' from A_train DataFrame
    A_test_features = A_test['Cleaned_Features']  # Access 'Cleaned_Features' from A_test DataFrame

    # Train and evaluate the model for gait_chest only
    results_hand, cm = train_and_evaluate_model(
        model_name, model, A_train_features, A_test_features, B_train, B_test
    )
    results_list_hand.append(results_hand)
    confusion_matrices[model_name] = cm

# Create a DataFrame for results
df_results_hand = pd.DataFrame(results_list_hand)
print("\nFinal Results Table:")
print(df_results_hand)

# Plot all confusion matrices in a 6x2 grid
num_models = len(confusion_matrices)
fig, axes = plt.subplots(3, 4, figsize=(18, 12))
axes = axes.flatten()

for idx, (model_name, cm) in enumerate(confusion_matrices.items()):
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[idx],
                xticklabels=['Predicted 0', 'Predicted 1'],
                yticklabels=['Actual 0', 'Actual 1'])
    axes[idx].set_title(f"Confusion Matrix: {model_name}")
    axes[idx].set_xlabel("Predicted")
    axes[idx].set_ylabel("Actual")

# Hide any unused subplots
for i in range(len(confusion_matrices), len(axes)):
    fig.delaxes(axes[i])

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Assuming 'df_results_hand' contains the results for all models, with columns like 'Accuracy', 'F1 Score', etc.

# Define short forms for the models
model_shortforms = {
    "Logistic Regression": "LogReg",
    "Support Vector Machine": "SVM",
    "K-Nearest Neighbors": "KNN",
    "Decision Tree": "DT",
    "Random Forest": "RF",
    "Naive Bayes": "NB",
    "XGBoost": "XGB",
    "AdaBoost": "AdaBoost",
    "Gradient Boosting": "GB",
    "Linear Discriminant Analysis": "LDA",
    "Quadratic Discriminant Analysis": "QDA",
    "LightGBM": "LGBM"
}

# Apply short forms to the model names in the results DataFrame
df_results_hand["Model"] = df_results_hand["Model"].map(model_shortforms)

# Set the seaborn style for better aesthetics
sns.set(style="whitegrid", palette="muted", font_scale=1.2)

# Define the metrics to plot
metrics = ["Accuracy", "F1 Score", "Precision", "Recall", "Mean Squared Error", "Log Loss"]

# Plotting bar plots for each metric
fig, axes = plt.subplots(2, 3, figsize=(18, 10))  # 2 rows, 3 columns of subplots
axes = axes.flatten()

# Loop through each metric to create a plot
for i, metric in enumerate(metrics):
    sns.barplot(x=df_results_hand["Model"], y=df_results_hand[metric], ax=axes[i], palette="viridis", edgecolor='black')
    axes[i].set_title(f"{metric} Comparison", fontsize=16, weight='bold')
    axes[i].set_xlabel("Model", fontsize=14, weight='bold')
    axes[i].set_ylabel(metric, fontsize=14, weight='bold')
    axes[i].set_ylim(0, 1)  # Setting y-axis limits for better comparison (for most metrics like accuracy, precision, etc.)

    # Rotate x-axis labels for better readability
    axes[i].tick_params(axis="x", rotation=45)

    # Adding horizontal gridlines
    axes[i].grid(True, axis='y', linestyle='--', alpha=0.7)

# Add overall figure title
fig.suptitle("Model Performance Comparison (Hand Data)", fontsize=18, weight='bold')

# Adjust layout to avoid overlap
plt.tight_layout(rect=[0, 0, 1, 0.96])  # Leave space for the suptitle
plt.show()

"""**Selecting the BEST ML Model for - Chest Hand Leg Gait Parameter**"""

import pandas as pd

# Function to determine the best model based on selected metrics
def rank_models_by_metrics(results_list_chest, results_list_hand, results_list_leg, metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Log Loss', 'Mean Squared Error']):
    # Combine results from all datasets (gait_chest, gait_hand, gait_leg)
    all_results = results_list_chest + results_list_hand + results_list_leg

    # Create a DataFrame from the results
    df_results = pd.DataFrame(all_results)

    # Create a score for each model based on the selected metrics
    # You may assign different weights for each metric based on their importance
    weights = {
        'Accuracy': 1,
        'Precision': 1,
        'Recall': 1,
        'F1 Score': 1,
        'Log Loss': -1,  # Negative because lower log loss is better
        'Mean Squared Error': -1  # Negative because lower MSE is better
    }

    # Normalize the metrics (if required) to make them comparable
    for metric in metrics:
        if metric == 'Log Loss' or metric == 'Mean Squared Error':
            # Lower values are better, so reverse the scale by multiplying by -1
            df_results[metric] = df_results[metric] * weights[metric]
        else:
            df_results[metric] = df_results[metric] * weights[metric]

    # Create a column for the total score (sum of weighted metrics)
    df_results['Total Score'] = df_results[metrics].sum(axis=1)

    # Sort models by Total Score in descending order (best model first)
    best_models = df_results.sort_values('Total Score', ascending=False)

    return best_models[['Model', 'Total Score']]

# Rank models based on metrics for each dataset (gait_chest, gait_hand, gait_leg)
best_models_chest = rank_models_by_metrics(results_list_chest, [], [], metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Log Loss', 'Mean Squared Error'])
best_models_hand = rank_models_by_metrics([], results_list_hand, [], metrics=['Accuracy', 'Precision', 'Recall', 'F1 Score', 'Log Loss', 'Mean Squared Error'])

# Print the top models for each dataset
print("Best Model for Gait Chest Dataset:")
print(best_models_chest)

print("\nBest Model for Gait Hand Dataset:")
print(best_models_hand)

"""**AdaBoost Proves to be the BEST Model Conclusively**

**Prediction Process**

**Gait - Chest Data**
"""

import joblib
joblib.dump(Model["AdaBoost"], 'AB_Gait_Chest.joblib')  # Save the actual model object
loaded_AD_Gait_Chest = joblib.load('AB_Gait_Chest.joblib')

"""0.188052, 0.935134, 0.230045, 0.016175, 0.927321, 0.396063, 0.093324, 0.960525, 0.335027, 0.029358, 0.933669, 0.429755, 0.099184, 0.985428, 0.122621, -0.010193, 0.882399, 0.492744, 0.006897, 0.93611, 0.391669, -0.095155, 0.870192, 0.5738, -0.018494, 0.895583, 0.537179, 0.029847, 0.839918, 0.609445, 0.808179, 0.007874, 0.628977, 0.820386, -0.008728, 0.622629, 0.700267, -0.073671, 0.740795, 0.165591, -0.188418, 1.004471, 0.41706, -0.179141, 0.913161, -0.032166, 0.938552, 0.303777, 0.044007, 0.959548, 0.313542, 0.142641, 0.868238, 0.511299, 0.411688, 0.092836, 0.934646, 0.136294, 0.947829, 0.312566, 0.286687, 0.95613, 0.084535, 0.256901, 0.607492, 0.794995, -0.089296, 0.752514, 0.672923, 0.151919, 0.554757, 0.807202, -0.545846, 0.491279, 0.715892, -0.059998, 0.326726, 0.96199, 0.903395, 0.420478, -0.098573, 0.911696, 0.392157, -0.061952, 0.969314, 0.249088, 0.138247, 0.726146, -0.28217, 0.674388, 0.970291, 0.117739, -0.234806, -0.277775, -0.024353, 0.985428, -0.256779, -0.007752, 1.000076, -0.42182, 0.345281, 0.864332, -0.389593, 0.349187, 0.889723, -0.501411, 0.435615, 0.777417, -0.267033, -0.174258, 0.982986, -0.251896, -0.158633, 0.982498, -0.219669, -0.16791, 0.994217, 0.953689, -0.078553, 0.359442, 1.012772, -0.017517, 0.029358, 0.954177, -0.083925, 0.315007, 0.784741, 0.145083, -0.582467, 0.772534, -0.453071, 0.484443, 0.004944, 0.99324, -0.019471, 0.165103, 0.829175, 0.544991, 0.08014, 1.031327, 0.061097, 0.044984, 0.902419, 0.279362, -0.094179, 0.892653, 0.148989, 0.384833, 0.671458, 0.647532, 0.393133, 0.487861, 0.798901, 0.039612, 0.972244, 0.239322, -0.025818, 0.965408, 0.063539, 0.072328, 0.849195, -0.351995, 0.122133, 0.916579, 0.400458, 0.131899, 0.942458, 0.26569, 0.203189, 0.870192, 0.46247, 0.19147, 0.887282, 0.460517, 0.298405, 0.865797, 0.439521, 0.188052, 0.815015, 0.266667, 0.261295, 1.136797, 0.480049, 0.090883, 0.876539, 0.503487, 0.153872, 0.913161, 0.401923, 0.202701, 0.934646, 0.409247, 0.22858, 0.856031, 0.478096, 0.272526, 0.727123, 0.616281, 0.442939, 0.670481, 0.637278, 0.737377, 0.360906, 0.612863, 0.481025, 0.215885, 0.888258, 0.241764, 0.942947, 0.473213, 0.231022, 0.913649, 0.343328, 0.161685, 0.898024, 0.447822, 0.165591, 0.936599, 0.289616, 0.510811, 0.1944, 0.865797, -0.043885, 0.309148, 0.989822, -0.551705, 0.356023, 0.790112, -0.054627, 1.01326, 0.502998, 0.873121, 0.190494, -0.415473, 0.889723, 0.224186, -0.355413, 1.000076, 0.017151, 0.077699, 0.977615, 0.242741, 0.054749, 0.958083, 0.071839, 0.313542, -0.569772, 0.331121, 0.77351, -0.554147, 0.362371, 0.778393, -0.378851, -0.197696, 0.924392, -0.523384, 0.460517, 0.744701, -0.488716, 0.466865, 0.777417, -0.269474, -0.133242, 0.987381, 0.049866, -0.284611, 0.988357, -0.054627, -0.251896, 1.01326, -0.376898, 0.523995, 0.808667, -0.289494, 0.538643, 0.824292, -0.175235, 0.474189, 0.899001, -0.337835, 0.537179, 0.796948, -0.442817, -0.213321, 0.894118, 0.049378, 0.982498, -0.036561, -0.212833, 0.125551, 1.015702, 0.060609, 0.967849, 0.207584, -0.04242, 0.972732, 0.212467, -0.40473, 0.840894, -0.303655"""

import numpy as np
import joblib

# Load the pre-trained model for gait_chest data
loaded_model = joblib.load('AB_Gait_Chest.joblib')  # Update the filename to match the trained model for gait_chest

# Get input from the user for gait features (replace these with the actual gait-related features)
feature = input("Enter Feature: ") # Removed object()

# Convert the input string to a list of floats
feature_list = [float(x) for x in feature.split(", ")]

# Create a 2D NumPy array for prediction with shape (1, number_of_features)
input_data = np.array([feature_list])  # Reshape to 2D

# Make the prediction
prediction = loaded_AD_Gait_Chest.predict(input_data)

# Print the prediction
if prediction[0] == 1:
    print("The patient is predicted to have Parkinson's disease.")
else:
    print("The patient is predicted to be healthy.")

"""-0.040467, -0.959426, 0.347723, 0.048402, -0.963821, 0.320867, 0.089906, -0.989212, 0.1944, -0.309026, -0.740673, 0.66511, -0.121523, -0.994583, 0.244205, -0.022889, -0.876417, -0.586374, -0.242618, -0.966262, 0.136294, 0.165591, -1.024857, 0.550851, 0.15729, -0.774365, 0.712474, 0.02887, -0.748974, 0.737865, -0.174258, -0.726513, -0.669383, -0.336858, -0.906691, 0.192447, -0.11664, -0.992142, 0.108949, -0.547799, -0.580514, 0.546456, 0.023499, -1.005325, 0.027893, -0.737255, -0.637644, 0.490791, 0.236393, -0.984817, 0.223697, -0.161563, -0.980911, -0.040467, -0.747997, -0.617624, 0.341863, 0.503975, -0.852003, -0.069764, 0.482002, -0.80415, -0.309514, 0.311101, -0.875929, 0.403388, 0.261295, -0.675242, 0.765209, 0.132387, -0.937453, -0.295354, -0.19379, -0.558541, -0.762646, -0.149355, -0.69868, -0.649363, -0.143008, -0.717723, -0.62739, -0.121523, -0.717723, -0.636179, 0.273014, 0.121157, -0.909621, 0.351141, 0.202701, -0.880812, 0.389227, 0.266667, -0.825635, 0.45417, 0.294011, -0.794873, 0.413153, 0.306218, -0.798779, -0.435004, -0.592233, -0.615671, -0.471626, -0.557076, -0.624949, -0.400336, -0.191836, -0.829541, -0.126406, -0.243107, -0.903761, -0.202579, -0.261662, -0.905226, -0.343206, -0.426215, -0.779248, -0.335393, -0.425238, -0.78999, -0.742138, -0.55903, -0.320256, -0.237247, -0.22797, -0.879835, -0.22211, -0.228946, -0.889113, -0.44477, -0.459419, 0.301335, -0.226993, -0.864698, 0.395087, 0.009339, -0.829541, -0.478462, 0.06891, -0.911574, 0.463935, 0.169497, -0.911086, 0.432197, -0.20209, -0.80415, 0.656809, 0.310613, -0.926223, 0.280339, -0.186465, -0.922316, 0.396551, -0.822705, -0.456489, 0.315984, 0.182193, -0.874952, 0.489815, -0.21088, -0.97847, 0.065003, 0.169986, -0.955032, -0.148867, 0.231022, -0.956497, 0.178286, -0.166445, -0.747997, 0.388739, -0.754345, -0.653269, 0.317449, -0.058534, -0.99263, 0.125063, 0.222721, -0.965774, -0.064881, 0.129946, -0.871046, 0.532784, -0.174746, -1.094194, 0.290105, 0.440497, -0.806104, -0.237736, 0.127504, -0.950637, 0.191958, 0.106996, -0.882277, -0.391058, -0.922805, -0.322698, 0.131899, 0.055726, -0.546822, -0.773388, -0.060487, -0.954055, 0.303288, 0.253483, -0.893019, -0.31635, -0.784131, -0.560006, 0.381903, -0.252384, -0.922316, -0.614206, -0.073182, -0.979446, -0.111757, 0.042542, -0.977493, -0.185489, 0.077211, -0.587839, -0.743603, 0.242252, -0.374945, -0.854444, 0.226139, -0.555612, -0.737743, -0.78706, -0.440375, 0.483467, 0.709056, -0.546822, -0.396918, 0.6822, -0.624949, -0.328069, 0.708568, -0.572702, -0.340764, 0.693431, -0.604929, -0.314885, -0.124941, -0.523384, -0.779248, -0.059022, -0.632273, -0.700633, 0.081117, -0.723095, -0.601999, 0.158267, -0.724071, -0.597604, 0.323308, -0.754833, -0.508248, 0.776928, -0.026307, -0.56733, -0.508736, -0.584909, -0.554635, -0.518502, -0.577584, -0.569772, -0.504341, -0.588815, -0.56733, 0.676829, -0.675242, -0.226505, -0.612253, -0.540475, -0.517525, -0.368109, -0.419379, -0.760204, -0.150332, -0.728954, -0.602487, -0.094179, -0.711376, -0.63032, 0.200259, -0.936965, 0.372137, -0.181094, -0.699168, 0.760327, 0.367254, -0.907668, 0.357488, -0.473091, -0.596628, 0.621164, -0.089784, -0.237247, -0.903761"""

import numpy as np
import joblib

# Load the pre-trained model for gait_chest data
loaded_model = joblib.load('AB_Gait_Chest.joblib')  # Update the filename to match the trained model for gait_chest

# Get input from the user for gait features (replace these with the actual gait-related features)
feature = input("Enter Feature: ") # Removed object()

# Convert the input string to a list of floats
feature_list = [float(x) for x in feature.split(", ")]

# Create a 2D NumPy array for prediction with shape (1, number_of_features)
input_data = np.array([feature_list])  # Reshape to 2D

# Make the prediction
prediction = loaded_AD_Gait_Chest.predict(input_data)

# Print the prediction
if prediction[0] == 1:
    print("The patient is predicted to have Parkinson's disease.")
else:
    print("The patient is predicted to be healthy.")

"""**Gait - Hand Data**"""

import joblib
joblib.dump(Model["AdaBoost"], 'AB_Gait_Hand.joblib')  # Save the actual model object
loaded_AD_Gait_Hand = joblib.load('AB_Gait_Hand.joblib')

"""0.036194, -0.61323, 0.823804, 0.380926, -0.705028, 0.628977, 0.094789, -0.5019, 0.8907, 0.04889, -0.81001, 0.620676, 0.286687, -0.859815, 0.446357, -0.18793, -0.844678, 0.586007, 0.797436, -0.6645, 0.237369, 0.229068, -0.976516, 0.162173, -0.175723, -0.843214, 0.591867, 0.105043, -0.974563, 0.250553, 0.426337, -0.812451, 0.421942, 0.100649, -0.372503, 0.922438, 0.90779, -0.217227, 0.293523, 0.835523, -0.534615, -0.040467, -0.29926, -0.948684, 0.194888, -0.2934, -0.791943, 0.597726, 0.21149, -0.325628, 0.916091, 0.235904, -0.588815, 0.811597, 0.368719, -0.957473, 0.284733, 0.886793, -0.456001, 0.010315, -0.023377, -1.038529, 0.73933, -0.035584, -0.896925, 0.466377, 0.736889, -0.003845, -0.65864, 0.72517, 0.137758, 0.710033, 0.733471, 0.265202, 0.627024, 0.755444, 0.094789, 0.669505, 0.804273, 0.334539, 0.502022, 0.730053, 0.336004, 0.617258, -0.11664, -0.02533, 1.018631, -0.087831, -0.977005, 0.210025, 0.411688, -0.688426, 0.631418, 0.369207, -0.662058, 0.674876, 0.389715, -0.582956, 0.738842, 0.616281, 0.762768, 0.165591, 0.271061, 0.958572, 0.156313, 0.595285, 0.596262, -0.51899, 0.64509, 0.748119, -0.129335, 0.639231, 0.729564, -0.156191, 0.71931, 0.666087, -0.154726, -0.393988, -0.920363, -0.088319, 0.946853, -0.203555, -0.165469, 0.055726, -1.029252, 0.082094, -0.596139, -0.905715, 0.106996, -0.298283, -0.955032, 0.162173, -0.24506, -0.929641, 0.401923, 0.055238, -0.907179, 0.303288, -0.457954, -0.750927, 0.402899, -0.941848, -0.693797, 0.252018, -0.831983, -0.278752, 0.345769, -0.468208, -0.757275, 0.534249, 0.139223, -0.839307, 0.516182, 0.323796, -0.870558, -0.124453, 0.771557, -0.478462, -0.366644, -0.830518, -0.290471, 0.480049, -0.482856, -0.880812, 0.02887, -0.209903, -0.976028, 0.088441, -0.313909, -0.936965, 0.16022, -0.010681, -0.856397, 0.557687, -0.860792, -0.487251, 0.324773, 0.698314, -0.68159, -0.009705, -0.271427, -0.967239, -0.0224, -0.484321, -0.890578, 0.029358, 0.250065, -0.770459, 0.619211, 0.69099, 0.710033, -0.172305, 0.689036, 0.713451, -0.041444, 0.391669, 0.210513, -0.879347, 0.455634, 0.260319, -0.819776, 0.574289, 0.275944, -0.755322, 0.49958, 0.756909, -0.401312, 0.293523, 0.349187, -0.857374, 0.064027, -0.94966, 0.310124, 0.775952, -0.608835, 0.023987, 0.903883, -0.411566, 0.077699, -0.31928, -0.921828, 0.328679, -1.382284, -0.591257, 0.635325, -0.307073, -0.950149, 0.182681, -0.396918, -0.909621, 0.115785, -0.307561, -0.948684, 0.136294, -0.251896, -0.96968, 0.185122, -0.348577, -0.929152, 0.106508, -0.577584, -0.823682, 0.392645, 0.608957, 0.744701, -0.268009, 0.567941, 0.816968, 0.022034, 0.577707, 0.817456, 0.009827, 0.523018, 0.832593, -0.200137, 0.628489, 0.667063, 0.385809, 0.703197, 0.693431, 0.085023, 0.761791, 0.60798, -0.183047, 0.4112, -0.181094, 0.924392, -0.084413, 0.166567, 1.013748, -0.139101, 0.156802, 1.022538, 0.136782, -0.055604, -0.954543, 0.118715, 0.017639, -0.982376, -0.024842, 0.422919, 0.935622, 0.937087, -0.328557, 0.063539, -0.238712, -0.916457, 0.37116, 0.724681, -0.623484, -0.257267, -0.402777, -0.89546, 0.261784, -0.26508, -0.92134, 0.349187, -0.369574, -0.895949, 0.256413"""

import numpy as np
import joblib

# Load the pre-trained model for gait_chest data
loaded_model = joblib.load('AB_Gait_Hand.joblib')  # Update the filename to match the trained model for gait_chest

# Get input from the user for gait features (replace these with the actual gait-related features)
feature = input("Enter Feature: ") # Removed object()

# Convert the input string to a list of floats
feature_list = [float(x) for x in feature.split(", ")]

# Create a 2D NumPy array for prediction with shape (1, number_of_features)
input_data = np.array([feature_list])  # Reshape to 2D

# Make the prediction
prediction = loaded_AD_Gait_Hand.predict(input_data)

# Print the prediction
if prediction[0] == 1:
    print("The patient is predicted to have Parkinson's disease.")
else:
    print("The patient is predicted to be healthy.")

"""0.577707, -0.811475, 0.540108, -0.417914, -0.463813, 0.847242, -0.997513, -0.363226, 0.368231, 0.817456, -0.585397, 0.101137, -0.155703, -0.825635, 0.591379, 0.269108, -0.436957, 0.912673, -0.209415, -0.871534, 0.432197, 0.59089, -0.478462, 0.683665, 0.370184, -0.640574, 0.68806, -0.000427, -1.000443, 0.116762, 0.179263, -0.087343, -0.945266, 0.162661, -0.310491, 0.966384, -0.010193, -0.362737, 0.983474, -0.194278, -0.987747, 0.148013, -0.079042, -0.903273, 0.477607, 0.118715, -0.898878, -0.392523, 0.975662, -0.138613, 0.185122, 0.828687, -0.471138, 0.33698, 0.349676, -0.930617, 0.135805, 0.836988, 0.50837, 0.21149, -0.366155, 0.285222, -0.867628, -0.355413, 0.048402, -0.917433, -0.352972, -0.076112, -0.914015, -0.298772, -0.176699, -0.915969, -0.32221, -0.20209, -0.889113, -0.248966, -0.559518, -0.755322, -0.038514, -0.81001, -0.528267, 0.476631, -0.672801, -0.541451, 0.564523, -0.643015, -0.492622, 0.548409, -0.653269, -0.488228, 0.69099, -0.323674, -0.649363, 0.732982, -0.503365, -0.432074, 0.186587, -0.197208, 1.007401, 0.104067, 0.489326, 0.900465, -0.488228, -0.716258, 0.278386, -0.294377, -0.416938, -0.839307, 0.676341, -0.519478, -0.492134, 0.622141, -0.68159, -0.33637, 0.459541, -0.460883, 0.798413, -0.369574, -0.327581, -0.845167, -0.102968, -0.852491, 0.498604, 0.033753, -0.99556, -0.105898, 0.46247, -0.734813, 0.568429, 0.319402, -0.691356, 0.568429, 0.980545, 0.176333, 0.023011, 0.523507, -0.690867, 0.606516, 0.402411, -0.687938, 0.623117, 0.021546, -0.736278, 0.737865, 0.218326, -0.922805, 0.387762, -0.185, -0.475532, 0.911696, -1.009232, 0.08014, 0.122133, 0.042542, -0.701122, 0.759838, -0.747021, 0.085023, 0.70515, 0.353582, -0.149355, 0.975662, -0.832959, -0.226993, 0.533272, 0.48249, -0.84419, 0.322332, -0.026307, -0.440864, 0.946853, -0.042908, -0.37055, 0.980545, 0.022034, -1.005325, -0.414984, -0.108827, -0.840772, -0.55024, 0.710033, 0.156802, 0.731029, -0.307073, -0.240177, 0.962966, 0.329656, -0.925246, 0.217838, 0.112367, -0.828565, 0.624582, 0.272526, -0.95552, 0.030823, 0.297917, 0.081117, 0.982498, 0.545968, -0.780224, -0.345647, 0.959548, -0.166445, -0.223087, 0.583078, -0.81001, 0.128481, -0.066835, -0.928176, 0.414618, 0.034241, -0.761669, 0.700267, 0.318914, -0.802686, -0.480415, -0.760693, -0.778271, 0.328679, -0.853468, -0.459419, -0.295354, 0.585519, -0.499947, 0.787671, 0.586984, -0.785107, -0.185, 0.68806, -0.707958, -0.141543, -0.288518, -0.522896, -0.798291, 0.767651, -0.104433, -0.606394, 0.883864, -0.250431, -0.340276, -0.434028, -0.843702, -0.323674, -0.446723, -0.268986, -0.828565, -0.159609, -0.553658, -0.794873, -0.341741, -0.565866, 0.844312, 0.235416, -0.858839, 0.510811, 0.003479, -0.908644, 0.477119, 0.638254, -0.622995, -0.446723, -0.47602, -0.431098, -0.75874, 0.713451, -0.617624, -0.31635, -0.101503, -0.514107, -0.798779, 0.77351, -0.466743, -0.40473, 0.624094, -0.731884, -0.247013, 0.48542, -0.559518, -0.641062, 0.078676, -0.81001, 0.625559, 0.383368, -0.908644, 0.205142, 0.324773, -0.813916, -0.467231, 0.732006, 0.674388, 0.135805, 0.294987, -1.041459, 0.271061, 0.444404, -0.68159, 0.626047, -0.20502, -0.88423, -0.400336"""

import numpy as np
import joblib

# Load the pre-trained model for gait_chest data
loaded_model = joblib.load('AB_Gait_Hand.joblib')  # Update the filename to match the trained model for gait_chest

# Get input from the user for gait features (replace these with the actual gait-related features)
feature = input("Enter Feature: ") # Removed object()

# Convert the input string to a list of floats
feature_list = [float(x) for x in feature.split(", ")]

# Create a 2D NumPy array for prediction with shape (1, number_of_features)
input_data = np.array([feature_list])  # Reshape to 2D

# Make the prediction
prediction = loaded_AD_Gait_Hand.predict(input_data)

# Print the prediction
if prediction[0] == 1:
    print("The patient is predicted to have Parkinson's disease.")
else:
    print("The patient is predicted to be healthy.")